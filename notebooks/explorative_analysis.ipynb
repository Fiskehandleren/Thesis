{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from utils.dataloader import load_data, create_count_data\n",
    "from utils.plotting_functions import plot_histograms, plot_time_series, ts_percentile\n",
    "#from utils.plotting_functions import plot_histograms, plot_time_series, ts_percentile\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "#plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "df.loc[df['Station Name'] == 'PALO ALTO CA / BRYANT # 1', 'Station Name'] =  'PALO ALTO CA / BRYANT #1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hub_name(station):\n",
    "   hub_name = station.rsplit('/', 1)[1].lstrip().rsplit(' ', 1)[0]\n",
    "   if hub_name == 'BRYANT #':\n",
    "      return 'BRYANT'\n",
    "   else: \n",
    "      return hub_name\n",
    "\n",
    "def count_plugs(station, station_list, no_plug_list):\n",
    "   no_plugs = len(df['Port Number'].loc[df['Station Name'] == station].unique())\n",
    "   return no_plugs\n",
    "   '''if 'SHERMAN' in station:\n",
    "      return 2\n",
    "   else: \n",
    "      return no_plugs'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plugs_usage = df.groupby(['Station Name', 'Port Number']).count()\n",
    "df_plugs_usage = df_plugs_usage.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plugs_usage['Station_Name_Port_Number'] = df_plugs_usage['Station Name'] + '_PortNo:' + df_plugs_usage['Port Number'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plugs_usage.sort_values('Id', ascending=False).plot.bar(x='Station_Name_Port_Number', y = 'Id', figsize=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add hub colu,n\n",
    "df['Hub'] = df['Station Name'].apply(create_hub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_num = []\n",
    "station_name = []\n",
    "j = 0\n",
    "for i in df['Station Name'].unique():\n",
    "    station_name.append(i)\n",
    "    df_port_num = df['Port Number'].loc[df['Station Name'] == i].unique()\n",
    "    port_num.append(int(len(df_port_num)))\n",
    "\n",
    "plug_per_station = dict(zip(station_name, port_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Poulate variable no. of plugs with plugs pr. unique station\n",
    "df['No. Plugs'] = np.zeros(len(df))\n",
    "for key, value in plug_per_station.items():\n",
    "    df.loc[df['Station Name'] == key, ['No. Plugs']] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find first time a specific plug at a specific station is in use\n",
    "list_of_first_records = []\n",
    "for i in df['Station Name'].unique():\n",
    "    list_of_first_records.append(df[df['Station Name'] == i].iloc[0].values)\n",
    "df_charging_stations = pd.DataFrame(list_of_first_records)\n",
    "df_charging_stations = df_charging_stations.set_axis(df.columns.unique().values, axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count no. of charging stations pr. hub over time\n",
    "df_charging_stations['No. Stations'] = df_charging_stations.groupby('Hub').cumcount()\n",
    "df_charging_stations['No. Stations'] = df_charging_stations['No. Stations'] + 1\n",
    "df_charging_stations['No. Plugs per Hub'] = df_charging_stations.groupby('Hub').cumsum()['No. Plugs']\n",
    "df_charging_stations_per_hub = df_charging_stations[['Station Name', 'Start Date', 'Port Number', 'Hub', 'No. Stations', 'No. Plugs per Hub']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create df with time and hubs as rows and columns respectively. Filled with number of charging stations\n",
    "df_hub = df_charging_stations_per_hub.pivot_table(values='No. Stations', index=['Start Date'], columns='Hub', aggfunc='first')\n",
    "df_hub = df_hub.fillna(0)\n",
    "df_hub = df_hub.replace(to_replace=0, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plug_hub = df_charging_stations_per_hub.pivot_table(values='No. Plugs per Hub', index=['Start Date'], columns='Hub', aggfunc='first')\n",
    "df_plug_hub = df_plug_hub.fillna(0)\n",
    "df_plug_hub = df_plug_hub.replace(to_replace=0, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, figsize=(16,12), sharex=True)\n",
    "df_hub.plot(colormap = 'Paired', linewidth = 3, title = 'No. of stations pr. hub', ax=axes[0])\n",
    "df_plug_hub.plot(colormap = 'Paired', linewidth = 3, title = r'No. of plugs pr. hub ($\\tau$)', ax=axes[1])\n",
    "# make x axis integer\n",
    "axes[1].yaxis.set_major_locator(plt.MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plug_hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring distributions from when the models begin recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read count data format\n",
    "df_event = pd.read_csv('../data/charging_session_count_30_viz.csv', index_col=0, parse_dates=['Period'])\n",
    "\n",
    "#df_event = create_count_data(df, interval_length = 30, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting first record of each station \n",
    "first_date_rec = []\n",
    "j = 0\n",
    "\n",
    "for i in df_charging_stations['Hub'].unique():\n",
    "    first_recording = df_charging_stations['Start Date'].loc[df_charging_stations['Hub'] == i].iloc[0]\n",
    "    first_date_rec.append(first_recording)\n",
    "    \n",
    "station_name = df_event.columns.values\n",
    "hub_start_dict = dict(zip(station_name, first_date_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_list = df_event.columns.values\n",
    "def plot_histograms(df_in, plot_best_estimate = False, save_figure = False):\n",
    "    from scipy.optimize import curve_fit\n",
    "    from scipy.stats import poisson\n",
    "    \n",
    "    plot_list = df_in['Cluster'].unique()[:-1]\n",
    "    \n",
    "    fig, axes = plt.subplots(4,2, figsize=(12, 16))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for key in plot_list:\n",
    "        cluster_events = df_in['Sessions'].loc[df_in['Cluster'] == key]\n",
    "        cluster_mean = np.mean(df_in['Sessions'].loc[df_in['Cluster'] == key])\n",
    "        cluster_sd = np.std(df_in['Sessions'].loc[df_in['Cluster'] == key])\n",
    "\n",
    "        #cluster_events = df_event[key].values\n",
    "        #cluster_mean = np.mean(cluster_events)\n",
    "        #cluster_sd = np.std(cluster_events)\n",
    "\n",
    "        entries, bin_edges, patches  = axes[i,j].hist(cluster_events, bins = range(int(np.max(cluster_events))+3), \n",
    "                                                      rwidth=0.7, density = True)\n",
    "        axes[i,j].set_title(str(key))\n",
    "        axes[i,j].plot()\n",
    "        axes[i,j].set_xticks(np.arange(np.max(cluster_events)+3) + 0.5)\n",
    "        axes[i,j].set_xticklabels(np.arange(np.max(cluster_events)+3))\n",
    "        \n",
    "        textstr = '\\n'.join((\n",
    "        r'$\\mu=%.2f$' % (cluster_mean, ),\n",
    "        r'$\\sigma=%.2f$' % (cluster_sd, )))\n",
    "\n",
    "        if (plot_best_estimate == True):\n",
    "            \n",
    "            # calculate bin centers\n",
    "            middles_bins = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n",
    "\n",
    "            def fit_function(k, lamb):\n",
    "                # The parameter lamb will be used as the fit parameter\n",
    "                return poisson.pmf(k, lamb)\n",
    "\n",
    "\n",
    "            # fit with curve_fit\n",
    "            parameters, cov_matrix = curve_fit(fit_function, middles_bins, entries)\n",
    "\n",
    "            # plot poisson-deviation with fitted parameter\n",
    "            x_plot = np.arange(0, np.max(cluster_events)+3)\n",
    "\n",
    "            axes[i,j].plot(\n",
    "                x_plot,\n",
    "                fit_function(x_plot, *parameters),\n",
    "                marker='D', linestyle='-',\n",
    "                color='red',\n",
    "                label='Fit result',\n",
    "            )\n",
    "            textstr = '\\n'.join([textstr,\n",
    "            r'$\\lambda=%.2f$' % (parameters, )])     \n",
    "    \n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        axes[i,j].text(0.83, 0.962, textstr, transform=axes[i,j].transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "        \n",
    "        i += 1\n",
    "        if i == 4:\n",
    "            j += 1\n",
    "            i = 0\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if (save_figure == True):\n",
    "        fig.savefig(\"../Figures/Distributions.png\", bbox_inches='tight')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Could attempt to find best fitted pdf to above distributions\n",
    "plot_histograms(df_event, plot_best_estimate = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily and weekly patterns \n",
    "#### Using quantile plots to explore patterns across clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add day and hour variable\n",
    "df_event['Day'] = df_event['Period'].dt.dayofweek\n",
    "df_event['Hour'] = df_event['Period'].dt.hour\n",
    "plot_list = df_event['Cluster'].unique()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tsplot_v2(df, n=2, time_scale = 24, Cluster = 'HAMILTON', value = 'Sessions', percentile_min=10, percentile_max=90, color='r', plot_mean=False, plot_median=True, line_color='k', ax1 = 0, ax2 = 0, plt_axes = axes, **kwargs):\n",
    "    x = np.arange(0,time_scale)\n",
    "\n",
    "    # calculate the lower and upper percentile groups, skipping 50 percentile\n",
    "    perc1 = np.zeros((n, time_scale))\n",
    "    perc2 = np.zeros((n, time_scale))\n",
    "\n",
    "    if time_scale == 7:\n",
    "        group_var = 'Day'\n",
    "    else: \n",
    "        group_var = 'Hour'\n",
    "\n",
    "    label_list = []\n",
    "    j = 0\n",
    "    for i in np.linspace(percentile_min, 50, n)[:-1]:\n",
    "        perc1[j,:] = df[df['Cluster'] == Cluster].groupby(group_var).agg([percentile(i)])[value].values.flatten()\n",
    "        label_list.append(i)\n",
    "        j +=1\n",
    "    \n",
    "    k = 0\n",
    "    for i in np.linspace(50, percentile_max, n)[1:]:\n",
    "        perc2[k,:] = df[df['Cluster'] == Cluster].groupby(group_var).agg([percentile(i)])[value].values.flatten()\n",
    "        label_list.append(i)\n",
    "        k +=1\n",
    "\n",
    "    if 'alpha' in kwargs:\n",
    "        alpha = kwargs.pop('alpha')\n",
    "    else:\n",
    "        alpha = 1/n\n",
    "    # fill lower and upper percentile groups\n",
    "    p = 0\n",
    "    for p1, p2 in zip(perc1, perc2):\n",
    "        if (p == n-1):\n",
    "            label_name = '_nolegend_'\n",
    "        else:\n",
    "            label_name = f'{label_list[p]:.0f}-{100-label_list[p]:.0f}% percentile'\n",
    "        axes[ax1, ax2].fill_between(x, p1, p2, alpha=alpha, color=color, edgecolor=None, label = label_name)\n",
    "        p += 1\n",
    "\n",
    "    if plot_mean:\n",
    "        axes[ax1, ax2].plot(x, df[df['Cluster'] == Cluster].groupby(group_var).mean()[value].values, color=line_color, label = 'Mean')\n",
    "\n",
    "    if plot_median:\n",
    "        axes[ax1, ax2].plot(x, df[df['Cluster'] == Cluster].groupby(group_var).median()[value].values, color=line_color)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,2, figsize=(12, 18), sharey = True)\n",
    "i = 0\n",
    "j = 0\n",
    "n_percentile = 3\n",
    "for key in plot_list:\n",
    "    tsplot_v2(df_event, n=n_percentile, time_scale = 24, Cluster = key, value = 'Sessions', percentile_min=10, percentile_max=90, plot_median=True, plot_mean=False, color='g', line_color='navy', ax1 = i, ax2 = j, plt_axes = axes)\n",
    "    axes[i,j].set_title(key)\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        j += 1\n",
    "        i = 0\n",
    "\n",
    "legend = axes[0,0].legend(loc=\"upper center\", fontsize=12, markerscale=2, frameon=False)\n",
    "handles = legend.legendHandles\n",
    "\n",
    "# There are many more hatches available in matplotlib\n",
    "for i, handle in (enumerate(handles)):\n",
    "    handle.set_alpha((i+0.5)*(1/n_percentile))\n",
    "    handle.set_color('green')\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "#props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "#axes[i,j].text(0.5, 0.5, textstr, transform=axes[i,j].transAxes, fontsize=14, verticalalignment='top', bbox=props)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consum = df[['Start Date', 'Cluster', 'Energy (kWh)']]\n",
    "df_consum['Day'] = df_consum['Start Date'].dt.dayofweek\n",
    "df_consum['Hour'] = df_consum['Start Date'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,2, figsize=(12, 18), sharey = True)\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for key in plot_list:\n",
    "    tsplot_v2(df_consum, n=4, time_scale = 24, Cluster = key, value = 'Energy (kWh)', percentile_min=10, percentile_max=90, plot_median=True, plot_mean=False, color='g', line_color='navy', ax1 = i, ax2 = j, plt_axes = axes)\n",
    "    axes[i,j].set_title(key)\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        j += 1\n",
    "        i = 0\n",
    "\n",
    "legend = axes[0,0].legend(loc=\"upper center\", fontsize=12, markerscale=2, frameon=False)\n",
    "handles = legend.legendHandles\n",
    "\n",
    "# There are many more hatches available in matplotlib\n",
    "for i, handle in (enumerate(handles)):\n",
    "    handle.set_alpha((i+0.5)*(1/n_percentile))\n",
    "    handle.set_color('green')\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.savefig('Figures/ConsumptionPercentiles.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df_event.groupby(['Cluster','Day']).median()['Sessions']\n",
    "df_weekly_jittered = df_weekly + 0.05*rng.standard_normal(df_weekly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12,6))\n",
    "for key in df_event['Cluster'].unique():\n",
    "    axes.plot(df_weekly_jittered[key], '-o', label = key)\n",
    "    axes.set_xlabel('Day')\n",
    "    axes.set_ylabel('Events')\n",
    "    axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding max no. plugs for each cluster at any time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv('../data/charging_session_count_30_viz.csv', index_col=0, parse_dates=['Period'])\n",
    "df_event['Day'] = df_event['Period'].dt.dayofweek\n",
    "df_event['Hour'] = df_event['Period'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create start and end date from plugs table\n",
    "## Uncomment below if name change is necessary\n",
    "df_charging_stations_per_hub = df_charging_stations_per_hub.rename(columns = {'End Date': 'EndDate', 'Start Date': 'StartDate'})\n",
    "df_charging_stations_per_hub['EndDate'] = df_charging_stations_per_hub['StartDate'].shift(-1, fill_value= df_event['Period'].iloc[-1])\n",
    "df_charging_stations_per_hub = df_charging_stations_per_hub.rename(columns = {'Hub': 'Cluster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charging_stations_per_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge dataframes containing events and the total number of plugs per cluster over time\n",
    "#df_censored = pd.merge(df_event, df_charging_stations_per_hub, how='inner', left_on='Cluster', right_on='Cluster')\n",
    "#df_censored = df_censored[(df_censored['Period']>=df_censored['StartDate']) & (df_censored['Period']<=df_censored['EndDate'])]\n",
    "#df_censored = df_censored[['Period', 'Sessions', 'Cluster', 'Day', 'Hour', 'No. Plugs per Hub']]\n",
    "#df_censored = df_censored.rename(columns =  {'No. Plugs per Hub': 'PlugCap'})\n",
    "def assignPlugCap(df_in, df_charging_plugs):\n",
    "    df_out = df_in.copy()\n",
    "    \n",
    "\n",
    "    ## Fix zero values in beginning\n",
    "    df_charging_plugs.iloc[0]['StartDate'] = '2011-07-29 20:00:00'\n",
    "\n",
    "    for i in np.arange(len(df_charging_plugs)):\n",
    "        df_out.loc[(df_out['Cluster'] == df_charging_plugs['Cluster'].iloc[i]) & \n",
    "                                        (df_out['Period'] >= df_charging_plugs['StartDate'].iloc[i]) & \n",
    "                                        (df_out['Period'] <= df_charging_plugs['EndDate'].iloc[i]),\n",
    "                                        'PlugCap'] = df_charging_plugs['No. Plugs per Hub'].iloc[i]\n",
    "\n",
    "    #df_out.sort_values(['Cluster','PlugCap']).groupby('Cluster').ffill()\n",
    "    df_out = df_out.fillna(method='ffill')\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_v2 = assignPlugCap(df_event, df_charging_stations_per_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_censored_v2.to_csv(\"/Users/julian/Documents/GitHub/Thesis/data/charging_session_count_30_capped_viz.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sessions and caps scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored = pd.read_csv(\"../data/charging_session_count_30_capped.csv\", index_col=0, parse_dates=['Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given old layout, use this function\n",
    "plot_time_series(df_censored, save_figure=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new datasets (1 minute and 30 minute resolutions) and plot time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF creating for 1 minute interval, use thiss\n",
    "df_censored = pd.read_csv(\"/Users/julian/Documents/GitHub/Thesis/data/charging_session_count_1.csv\", index_col=0, parse_dates=['Period'])\n",
    "\n",
    "# If creating for 30 minute interval, use this\n",
    "#df_censored = pd.read_csv(\"/Users/julian/Documents/GitHub/Thesis/data/charging_session_count_30_viz.csv\", index_col=0, parse_dates=['Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign plug cap\n",
    "df_censored_minute_cap = assignPlugCap(df_censored, df_charging_stations_per_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_to_input(df_in, write_to_csv = False):\n",
    "    minute_resolution = False     \n",
    "    \n",
    "    if (len(df_in) > 1500000):\n",
    "        minute_resolution = True\n",
    "\n",
    "    df_pivot_sessions = df_in.pivot_table(index='Period', columns='Cluster', values='Sessions')\n",
    "    df_pivot_plugCap = df_in.pivot_table(index='Period', columns='Cluster', values='PlugCap')\n",
    "\n",
    "    # Cut timeseries off at the latest timepoint of the cluster with the earliest last timepoint\n",
    "    df_pivot_sessions_reduced = df_pivot_sessions.loc[:df_censored.groupby('Cluster').agg({'Period': 'max'}).Period.min()].copy()\n",
    "    df_pivot_plugCap_reduced = df_pivot_plugCap.loc[:df_censored.groupby('Cluster').agg({'Period': 'max'}).Period.min()].copy()\n",
    "    # Cluster \"SHERMAN\" has no data before late 2021, so we drop it\n",
    "    df_pivot_sessions_reduced.drop('SHERMAN', axis=1, inplace=True)\n",
    "    df_pivot_plugCap_reduced.drop('SHERMAN', axis=1, inplace=True)\n",
    "    \n",
    "    # Fill missing values with 0\n",
    "    df_pivot_sessions_reduced.fillna(0, inplace=True)\n",
    "    df_pivot_plugCap_reduced.fillna(0, inplace=True)\n",
    "\n",
    "    ## Rename columns to cap for no. of plugs within cluster\n",
    "    df_pivot_plugCap_reduced = df_pivot_plugCap_reduced.rename(columns = {'BRYANT': 'BRYANT_CAP', 'CAMBRIDGE': 'CAMBRIDGE_CAP', 'HAMILTON':'HAMILTON_CAP',\n",
    "                                            'HIGH': 'HIGH_CAP', 'MPL': 'MPL_CAP', 'RINCONADA': 'RINCONADA_CAP', 'TED': 'TED_CAP',\n",
    "                                            'WEBSTER':'WEBSTER_CAP'})\n",
    "    \n",
    "    ## Merge datasets on index\n",
    "    df_sessions_cap = df_pivot_sessions_reduced.join(df_pivot_plugCap_reduced)\n",
    "\n",
    "    if (minute_resolution == True):\n",
    "        df_sessions_cap['Period'] = df_sessions_cap.index.values\n",
    "        df_sessions_cap = df_sessions_cap.resample('30T', on='Period').max()\n",
    "        df_sessions_cap = df_sessions_cap.drop(columns = ['Period'])\n",
    "        out_name = \"1_to_30\"\n",
    "\n",
    "    if (write_to_csv == True):\n",
    "        out_name = \"30\"\n",
    "        df_sessions_cap.to_csv('/Users/julian/Documents/GitHub/Thesis/data/charging_session_count_{out_name}_capped.csv')\n",
    "\n",
    "    return df_sessions_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constant censoring \n",
    "df_censored_cap = pivot_to_input(df_censored_minute_cap, write_to_csv = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap = pd.read_csv(\"/Users/julian/Documents/GitHub/Thesis/data/charging_session_count_1_to_30_capped.csv\", index_col=0, parse_dates=['Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_2(df_in, save_figure = False, is_censored = False, censor_scheme = 1):\n",
    "    plot_list_sessions = df_in.columns[0:8].values\n",
    "    plot_list_capacity = df_in.columns[8:].values\n",
    "\n",
    "    if (is_censored == True):\n",
    "        plot_list_censored = plot_list_sessions + '_CENSORED'\n",
    "\n",
    "    fig, axes = plt.subplots(4,2, figsize=(12, 14), sharey = True, sharex = True)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for key in np.arange(len(plot_list_sessions)):\n",
    "        session_name = plot_list_sessions[key]\n",
    "        cap_name = plot_list_capacity[key]\n",
    "\n",
    "        if (is_censored == True):\n",
    "            censored_name = plot_list_censored[key]\n",
    "            df_in.plot(y = [session_name, censored_name, cap_name], ax = axes[i,j], label = ['Sessions', r'$\\tau$', 'Plug capacity'])\n",
    "        else: \n",
    "            df_in.plot(y = [session_name, cap_name], ax = axes[i,j], label = ['Sessions', 'Plug capacity'])\n",
    "        \n",
    "        axes[i,j].set_title(session_name)\n",
    "        axes[i,j].legend(loc = 'upper left')\n",
    "\n",
    "        no_events = len(df_in)\n",
    "        no_exceed_plugcap =  1  #(len(df_in[df_in[session_name] > df_in[cap_name]]) / no_events)*100 (FIIX) \n",
    "        usage_fraction = 1    #np.mean(df_in[session_name] / df_in[cap_name])\n",
    "\n",
    "        textstr = '\\n'.join((\n",
    "        r'$\\eta =%.2f$' % (no_exceed_plugcap, ),\n",
    "        r'$\\mu =%.2f$' % (usage_fraction , )))\n",
    "\n",
    "        if (is_censored == True):\n",
    "            ## Index of first non-zero index (beginning of recording)\n",
    "            idx_nonzero = df_in.index.get_loc(df_in.ne(0).idxmax()[session_name])\n",
    "\n",
    "            ## Number of records of given cluster\n",
    "            no_records = len(df_in.iloc[idx_nonzero:])\n",
    "\n",
    "            ## Fraction of censored observations\n",
    "            no_censored = (len(df_in.iloc[idx_nonzero:][df_in.iloc[idx_nonzero:][session_name + '_IS_CENSORED'] == 0])) / no_records\n",
    "\n",
    "            ## Fraction of zero-valued observations\n",
    "            no_zero = len(df_in.iloc[idx_nonzero:][df_in.iloc[idx_nonzero:][session_name] == 0]) / no_records\n",
    "\n",
    "            #textstr = '\\n'.join((\n",
    "            #    r'$\\eta =%.2f$' % (no_censored, ),\n",
    "            #    r'$\\mu =%.2f$' % (no_zero, )))\n",
    "\n",
    "            textstr = r'$\\eta =%.2f$' % (no_censored, )\n",
    "            \n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        axes[i,j].text(0.855, 0.962, textstr, transform=axes[i,j].transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "        i += 1\n",
    "        if i == 4:\n",
    "            j += 1\n",
    "            i = 0\n",
    "    fig.tight_layout()\n",
    "    print(r'eta is the percentage of time intervals where the amount sessions exceed the number of plugs (due to time aggregation)')\n",
    "    #print(r'mu is the average fraction of plugs in use (out of the total number)')\n",
    "\n",
    "    if (save_figure == True):\n",
    "        fig.savefig(f\"../Figures/Time_series_{censor_scheme}.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 8002\n",
    "print(r'$\\eta = %2.f$' % (no, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series_2(df_censored_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap = df_censored_cap[['BRYANT', 'CAMBRIDGE', 'HAMILTON', 'HIGH', 'MPL', 'RINCONADA', 'TED', 'WEBSTER']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap.to_csv('../data/charging_session_count_1_to_30.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censoring variables dependent on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap = pd.read_csv('../data/charging_session_count_1_to_30_capped.csv', index_col=0, parse_dates=['Period'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_no_plugs(df_in):\n",
    "    ## According to the usage of a number of plugs, we see that the wrong number of plugs are sometimes reported. As a result,\n",
    "    ## we use this function to fix theses faults\n",
    "    df_in.loc[(df_in.index >= '2011-08-02 12:00:00'),'HAMILTON_CAP'] = 3.0\n",
    "    df_in.loc[(df_in.index >= '2015-01-02 08:30:00'),'RINCONADA_CAP'] = 3.0\n",
    "    df_in.loc[(df_in.index >= '2017-07-28 23:00:00'),'MPL_CAP'] = 4.0\n",
    "    df_in.loc[(df_in.index >= '2016-01-14 11:30:00'),'BRYANT_CAP'] = 5.0\n",
    "    df_in.loc[(df_in.index >= '2017-07-05 14:00:00'),'BRYANT_CAP'] = 9.0\n",
    "    df_in.loc[(df_in.index >= '2016-01-14 10:00:00'),'TED_CAP'] = 2.0\n",
    "    df_in.loc[(df_in.index >= '2018-03-21 08:15:00'),'TED_CAP'] = 8.0\n",
    "    \n",
    "    #df_in.loc[(df_in.index >= '2016-01-14 10:00:00') & (df_in.index <= '2018-03-21 7:30:00'), 'TED_CAP'] = 2.0\n",
    "\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap.loc[(df_censored_cap['MPL'] == 6),'MPL'].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap.loc[(df_censored_cap.index >= '2017-05-21 19:00:00'),'MPL'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_cap = fix_no_plugs(df_censored_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_censor(df_in, censor_threshold = [2,2,2,2,2,2,2,2], min_tot_plugs = [2,2,2,2,2,2,2,2], censor_scheme = 1, \n",
    "                  to_train_data = False, cap_observed_values = False):\n",
    "    ## df_in: df to copy and add aditional columns\n",
    "    ## censor_threshold: the number is subtracted from the total number of plugs. This will be the observed number of events from the model\n",
    "    ## min_session_observed: the minimum number of plugs that should be present, before censoring is applied\n",
    "\n",
    "    df_out = df_in.copy()\n",
    "\n",
    "    plot_list_sessions = df_in.columns[0:8].values\n",
    "    plot_list_capacity = df_in.columns[8:].values\n",
    "    df_size = len(df_out)\n",
    "\n",
    "    for i in np.arange(len(plot_list_sessions)): \n",
    "        session_name = plot_list_sessions[i]\n",
    "        cap_name = plot_list_capacity[i]  \n",
    "\n",
    "        if (cap_observed_values == True):\n",
    "            ## truncates observations to be maximum the number of available plugs\n",
    "            df_out.loc[(df_out[session_name]) > df_out[cap_name], session_name] = df_out[cap_name]      \n",
    "        \n",
    "        df_out[session_name + '_CENSORED'] = df_out[cap_name]\n",
    "        df_out[session_name + '_IS_CENSORED'] = np.ones(df_size)\n",
    "        \n",
    "        if (censor_scheme == 1):\n",
    "            ## Static censoring scheme: ones a user-defined threshold is reached, user-defined tau defines the maximum events that we can observe.\n",
    "            ## Any observed value equal to or above the threshold is a censored variable and the maximum value observed is tau\n",
    "            df_out.loc[(df_out[cap_name] > min_tot_plugs[i]), session_name + '_CENSORED'] =  censor_threshold[i]\n",
    "            df_out.loc[(df_out[session_name] >= df_out[session_name + '_CENSORED']), session_name + '_IS_CENSORED'] =  0\n",
    "\n",
    "            if (to_train_data == True):\n",
    "                ## If true, the observed value is changed to be the censored value. First, observed value is saved as a true value\n",
    "                ## then observed is changed to censored \n",
    "                df_out[session_name + '_TRUE'] = df_out[session_name]  #outcomment to remove true label\n",
    "                df_out.loc[(df_out[session_name + '_IS_CENSORED'] == 0), session_name] = df_out[session_name + '_CENSORED']\n",
    "                df_out[cap_name] = df_out[session_name + '_CENSORED']\n",
    "                \n",
    "                df_out = df_out.rename(columns = {cap_name: session_name + '_TAU'})\n",
    "                df_out = df_out.drop(columns = session_name + '_CENSORED')\n",
    "                df_out = df_out.drop(columns = session_name + '_IS_CENSORED')\n",
    "\n",
    "        elif(censor_scheme == 2):\n",
    "            ## Dynamic censoring scheme: ones a user-defined threshold is reached, a tau is defined as the maximum no. plugs subtracted \n",
    "            ## with a user-defined threshold, i.e. tau will dynammically change if the plug capacity of a hub is increased.\n",
    "            ## Any observed value equal to or above tau is a censored variable and the maximum value observed is tau\n",
    "            df_out.loc[(df_out[cap_name] > min_tot_plugs[i]), session_name + '_CENSORED'] =  (df_out[cap_name]-censor_threshold[i])\n",
    "            df_out.loc[(df_out[session_name] >= df_out[session_name + '_CENSORED']), session_name + '_IS_CENSORED'] =  0\n",
    "\n",
    "            if (df_out[cap_name].max() < min_tot_plugs[i])\n",
    "            \n",
    "            if (to_train_data == True):\n",
    "                ## If true, the observed value is changed to be the censored value\n",
    "                df_out[session_name + '_TRUE'] = df_out[session_name]  #outcomment to remove true label\n",
    "                df_out.loc[(df_out[session_name + '_IS_CENSORED'] == 0), session_name] = df_out[session_name + '_CENSORED']\n",
    "                df_out[cap_name] = df_out[session_name + '_CENSORED']\n",
    "                \n",
    "                df_out = df_out.rename(columns = {cap_name: session_name + '_TAU'})\n",
    "                df_out = df_out.drop(columns = session_name + '_CENSORED')\n",
    "                df_out = df_out.drop(columns = session_name + '_IS_CENSORED')\n",
    "\n",
    "            \n",
    "        elif(censor_scheme == 3):\n",
    "            ## random censoring given percentage \n",
    "            NotImplemented\n",
    "    \n",
    "\n",
    "\n",
    "       #(df_out[session_name] > (df_out[cap_name]-censor_threshold[i])),\n",
    "\n",
    "    \n",
    "    return df_out\n",
    "    \n",
    "    '''\n",
    "    if (to_train_data == True):\n",
    "        #df_out = df_out.drop(columns = plot_list_sessions + '_CENSORED')\n",
    "        #df_out = df_out.drop(columns = plot_list_sessions + '_IS_CENSORED')\n",
    "        #cols_to_update = [plot_list_sessions, plot_list_sessions + '_TAU']\n",
    "        #cols_to_update = np.concatenate(cols_to_update).ravel()\n",
    "\n",
    "        #df_out.columns = cols_to_update\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_censored_cap.columns[0:8].values)\n",
    "df_censored_train = static_censor(df_censored_cap, censor_threshold = [3,3,2,3,3,2,3,3], min_tot_plugs = [3,3,2,3,3,2,3,3],\n",
    "                                  censor_scheme = 1, to_train_data = False, cap_observed_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series_2(df_censored_train, is_censored = True, save_figure = False, censor_scheme = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_censored_train.to_csv('../data/charging_session_count_1_to_30_censored_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
